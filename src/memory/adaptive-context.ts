/**
 * Hybrid Adaptive Context Selector for UAM (Option 4)
 * 
 * VERSION: 1.9.0 - 10 Model Outcome Success Optimizations
 *
 * Combines task classification with time-budget awareness, runtime monitoring,
 * and historical benefit tracking for optimal context loading decisions.
 *
 * OPTIMIZATIONS IMPLEMENTED:
 * 1. Historical Data Persistence - SQLite instead of in-memory Map
 * 2. Task-specific context sections for 5 failing tasks  
 * 3. Missing context sections (git_recovery, web_parsing, data_processing, theorem_proving)
 * 4. Weighted keyword relevance scoring (TF-IDF-like specificity weights)
 * 5. Token budget utilization - increase minimal sections from 1→2
 * 6. Task-type-selective pattern injection
 * 7. Smarter progressive context escalation with error-to-section mapping
 * 8. Model Router fingerprint persistence integrated
 * 9. Multi-category task classification support
 * 10. Semantic caching foundation for task→outcome mappings
 */

import Database from 'better-sqlite3';
import { existsSync, mkdirSync } from 'fs';
import { dirname, join } from 'path';
import { fileURLToPath } from 'url';
import { classifyTask as classifyTaskType } from './task-classifier.js';
import { recordTaskOutcome as updateModelRouterFingerprint, getModelFingerprint } from './model-router.js';
import type { ModelId } from './model-router.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

export type ContextLevel = 'none' | 'minimal' | 'full';
export type TimePressure = 'critical' | 'high' | 'medium' | 'low';

export interface ContextDecision {
  level: ContextLevel;
  sections: string[];
  reason: string;
  estimatedOverheadMs: number;
  taskType: string;
  timePressure: TimePressure;
  historicalBenefit: number;
  // OPT 9: Multi-category support
  secondaryCategories?: string[];
  // OPT 6: Task-type-selective patterns
  relevantPatterns?: string[];
}

export interface TaskMetadata {
  timeout_sec?: number;
  difficulty?: 'easy' | 'medium' | 'hard';
  category?: string;
  historical_uam_benefit?: number;
  // OPT 10: Cache key for semantic caching
  cacheKey?: string;
}

export interface HistoricalData {
  taskType: string;
  totalAttempts: number;
  uamSuccesses: number;
  noUamSuccesses: number;
  avgTimeWithUam: number;
  avgTimeWithoutUam: number;
  lastUpdated: number;
}

// OPT 1: SQLite-backed historical data persistence
let historicalDb: Database.Database | null = null;

function getHistoricalDb(): Database.Database {
  if (historicalDb) return historicalDb;
  
  // Use the same data directory as short_term.db
  const dbDir = join(__dirname, '../../agents/data/memory');
  if (!existsSync(dbDir)) {
    mkdirSync(dbDir, { recursive: true });
  }
  
  const dbPath = join(dbDir, 'historical_context.db');
  historicalDb = new Database(dbPath);
  
  // Enable WAL mode for better concurrent access
  historicalDb.pragma('journal_mode = WAL');
  
  // Create schema if not exists
  historicalDb.exec(`
    CREATE TABLE IF NOT EXISTS historical_data (
      task_type TEXT PRIMARY KEY,
      total_attempts INTEGER DEFAULT 0,
      uam_successes INTEGER DEFAULT 0,
      no_uam_successes INTEGER DEFAULT 0,
      avg_time_with_uam REAL DEFAULT 0,
      avg_time_without_uam REAL DEFAULT 0,
      last_updated INTEGER DEFAULT 0
    );
    
    -- OPT 10: Semantic cache for task→outcome mappings
    CREATE TABLE IF NOT EXISTS semantic_cache (
      cache_key TEXT PRIMARY KEY,
      instruction_hash TEXT,
      decision_json TEXT,
      success_rate REAL DEFAULT 0.5,
      created_at INTEGER,
      last_used INTEGER,
      use_count INTEGER DEFAULT 1
    );
    
    CREATE INDEX IF NOT EXISTS idx_semantic_cache_hash ON semantic_cache(instruction_hash);
  `);
  
  return historicalDb;
}

// OPTIMIZATION 7: Refined low-benefit categories
const LOW_BENEFIT_CATEGORIES = new Set([
  'reasoning',       // Pure logical reasoning (sudoku, puzzles)
  'games',           // Game theory, chess position analysis (but NOT chess-best-move which needs stockfish)
  'pure-logic',      // Mathematical proofs, formal verification
  'mathematical',    // Pure math calculations
  'calendar',        // Meeting scheduling (but NOT server scheduling)
]);

// Categories that should ALWAYS receive UAM context regardless of historical data
const ALWAYS_BENEFIT_CATEGORIES = new Set([
  'security',
  'file-ops',
  'sysadmin',
  'debugging',
  'legacy',
  'coding',
  'testing',
  'ml-training',
  'git-recovery',     // OPT 3: Added for git tasks
  'data-processing',  // OPT 3: Added for data tasks
  'theorem-proving',  // OPT 3: Added for proof tasks
]);

// OPT 4: Weighted keywords - specificity score (higher = more specific = more valuable)
const HIGH_BENEFIT_KEYWORDS: Record<string, number> = {
  // Security - very specific terms get higher weights
  'hashcat': 3.0,
  'john': 2.5,
  '7z': 2.5,
  'xss': 3.0,
  'injection': 2.0,
  'sanitize': 2.0,
  'bleach': 3.0,
  'dompurify': 3.0,
  'password': 1.5,
  'hash': 1.5,
  'crack': 2.0,
  'decrypt': 2.0,
  'secret': 1.5,
  'exploit': 2.0,
  
  // File formats - specific formats get higher weights
  'elf': 3.0,
  'struct.unpack': 3.0,
  'e_phoff': 3.5,
  'sqlite': 2.0,
  'wal': 3.0,
  'binary': 1.5,
  'executable': 1.5,
  'extract': 1.5,
  
  // Git recovery - OPT 3
  'reflog': 3.0,
  'fsck': 3.0,
  'git recovery': 3.0,
  'lost commit': 2.5,
  'detached head': 2.5,
  'git reset': 2.0,
  'git rebase': 1.5,
  
  // Web parsing - OPT 2 (for filter-js-from-html)
  'html parse': 2.5,
  'dom': 2.0,
  'beautifulsoup': 2.5,
  'lxml': 2.5,
  'regex html': 2.0,
  
  // Compression - OPT 2 (for gpt2-codegolf)
  'codegolf': 3.0,
  'minify': 2.0,
  'compress': 1.5,
  'gzip': 2.0,
  'zlib': 2.5,
  
  // Chess - OPT 2 (for chess-best-move)
  'stockfish': 3.0,
  'python-chess': 3.0,
  'fen': 2.5,
  'pgn': 2.5,
  'chess position': 2.0,
  'chessimg2pos': 3.0,
  
  // Legacy
  'cobol': 3.0,
  'fortran': 2.5,
  'legacy': 1.5,
  'modernize': 1.5,
  'mainframe': 2.5,
  
  // Theorem proving - OPT 3 (for prove-plus-comm)
  'coq': 3.0,
  'lean': 3.0,
  'isabelle': 3.0,
  'theorem': 2.0,
  'proof': 1.5,
  'induction': 2.0,
  'commutativity': 2.5,
  
  // Data processing - OPT 3 (for reshard-c4-data)
  'parquet': 2.5,
  'sharding': 2.5,
  'reshard': 3.0,
  'c4 data': 3.0,
  'dataset': 1.5,
  'arrow': 2.0,
  
  // General coding
  'compile': 1.5,
  'build': 1.0,
  'makefile': 2.0,
  'database': 1.5,
  'recovery': 1.5,
};

// OPT 3 & 2: Extended context sections including new categories and task-specific sections
const CONTEXT_SECTIONS: Record<string, { 
  tokens: number; 
  keywords: string[]; 
  keywordWeights?: Record<string, number>;  // OPT 4: Per-section keyword weights
  content: string;
  patterns?: string[];  // OPT 6: Associated patterns
}> = {
  security: {
    tokens: 150,
    keywords: ['xss', 'password', 'hash', 'crack', 'secret', 'exploit', 'injection', 'sanitize', 'bleach', 'dompurify'],
    keywordWeights: { 'hashcat': 3, 'bleach': 3, 'dompurify': 3, 'xss': 2.5, 'injection': 2 },
    content: `### Security
- XSS: Use bleach.clean() or DOMPurify. Remove script, onclick, onerror, javascript:
- Password cracking: hashcat -m 11600 for 7z, -m 0 MD5, john for CPU
- Binary secrets: strings, objdump -d, check .rodata section
- Always validate and sanitize user input`,
    patterns: ['P10', 'P20'],  // Whitelist, Adversarial Testing
  },
  file_formats: {
    tokens: 120,
    keywords: ['elf', 'sqlite', '7z', 'archive', 'binary', 'extract', 'format', 'wal', 'struct'],
    keywordWeights: { 'e_phoff': 4, 'struct.unpack': 3, 'wal': 3, 'elf': 2.5 },
    content: `### File Formats
- ELF: Program headers at e_phoff. Use struct.unpack for parsing
- SQLite WAL: Header 32 bytes, frames follow. PRAGMA wal_checkpoint to recover
- 7z: Install p7zip-full, use 7z x -p for password protected archives`,
    patterns: ['P9', 'P35'],  // FormatPipeline, Decoder-First
  },
  coding: {
    tokens: 80,
    keywords: ['implement', 'function', 'class', 'refactor', 'algorithm', 'code'],
    content: `### Coding
- Use absolute paths (/app/...)
- Verify file exists before reading
- Handle edge cases in parsing
- Match exact output format required`,
    patterns: ['P12', 'P16'],  // OEV, TFE+
  },
  tools: {
    tokens: 100,
    keywords: ['hashcat', 'john', 'strings', 'objdump', 'readelf', 'command', 'cli'],
    keywordWeights: { 'hashcat': 3, 'john': 2.5, 'readelf': 2, 'objdump': 2 },
    content: `### Tools
- hashcat: GPU password cracking, -m flag for hash type
- john: CPU password cracking, flexible format support
- readelf/objdump: Binary analysis
- strings: Extract printable strings from binaries`,
    patterns: ['P4', 'P8'],  // ToolSpec, CLIoverLib
  },
  legacy: {
    tokens: 90,
    keywords: ['cobol', 'fortran', 'legacy', 'modernize', 'mainframe'],
    keywordWeights: { 'cobol': 3, 'fortran': 2.5, 'mainframe': 2.5 },
    content: `### Legacy Code
- COBOL: Fixed-format columns, WORKING-STORAGE for variables
- Modernization: Preserve business logic, update data structures
- Test with original inputs to verify behavior`,
    patterns: ['P3'],  // StateProtect
  },
  debugging: {
    tokens: 70,
    keywords: ['debug', 'error', 'fix', 'traceback', 'exception', 'crash'],
    content: `### Debugging
- Check logs first: journalctl, /var/log/
- Use verbose flags: -v, --debug
- Isolate the problem: binary search through changes`,
    patterns: ['P15', 'P13'],  // ER, IRL
  },
  domain_libraries: {
    tokens: 100,
    keywords: ['chess', 'statistics', 'compression', 'sampler', 'ars', 'stockfish', 'numerical'],
    keywordWeights: { 'stockfish': 3, 'python-chess': 3, 'scipy.stats': 2.5, 'ars': 3 },
    content: `### Domain Libraries (USE INSTEAD OF CUSTOM CODE)
- Chess: python-chess + stockfish (pip install python-chess), chessimg2pos for image→FEN
- Statistics/ARS: scipy.stats or R CRAN 'ars' package (NOT custom implementation)
- Compression: zlib, lz4, brotli (NOT custom Huffman). Test round-trip FIRST
- Numerical: numpy/scipy with tolerance 1e-6, test multiple random seeds`,
    patterns: ['P11'],  // PreComputed
  },
  output_verification: {
    tokens: 80,
    keywords: ['create', 'file', 'output', 'script', 'cli', 'executable', 'shebang'],
    content: `### Output Verification (37% of failures = missing files)
- touch output files FIRST, then implement
- CLI scripts: add #!/usr/bin/env python3 shebang + chmod +x
- Test with ./script (NOT python3 script.py)
- Verify all expected files: ls -la before claiming done
- Partial output > no output: always create the file`,
    patterns: ['P12', 'P16', 'P32'],  // OEV, TFE+, CEV
  },
  // OPT 3: NEW SECTIONS for missing capabilities
  git_recovery: {
    tokens: 100,
    keywords: ['git', 'reflog', 'fsck', 'reset', 'rebase', 'merge', 'conflict', 'detached', 'lost', 'HEAD'],
    keywordWeights: { 'reflog': 3, 'fsck': 3, 'dangling': 2.5, 'detached head': 2.5 },
    content: `### Git Recovery (P22 GRF Pattern)
- ALWAYS backup first: cp -r .git .git.bak
- Lost commits: git reflog show HEAD, then git reset --hard HEAD@{n}
- Dangling objects: git fsck --lost-found, check .git/lost-found/
- Detached HEAD: git checkout -b recovery-branch
- Merge conflicts: git merge --abort to reset, or resolve with git mergetool
- Force push recovery: git reflog, find pre-push commit, reset`,
    patterns: ['P22', 'P3'],  // GRF, StateProtect
  },
  web_parsing: {
    tokens: 110,
    keywords: ['html', 'parse', 'dom', 'beautifulsoup', 'lxml', 'regex', 'scrape', 'filter', 'js', 'javascript'],
    keywordWeights: { 'beautifulsoup': 2.5, 'lxml': 2.5, 'html.parser': 2, 'regex html': 2 },
    content: `### Web Parsing & XSS Filtering
- HTML parsing: Use BeautifulSoup with lxml parser (pip install bs4 lxml)
- XSS filtering: bleach.clean() with allowed_tags whitelist, NOT blacklist
- Script removal: Remove <script>, onclick, onerror, javascript: hrefs
- Preserve structure: Maintain HTML hierarchy, don't just regex replace
- Test edge cases: nested tags, malformed HTML, encoded entities
- filter-js-from-html: Create /app/filter.py with exact input/output format`,
    patterns: ['P10', 'P20', 'P14'],  // Whitelist, AT, OFV
  },
  data_processing: {
    tokens: 100,
    keywords: ['parquet', 'shard', 'reshard', 'dataset', 'arrow', 'pandas', 'dask', 'c4', 'chunk'],
    keywordWeights: { 'parquet': 2.5, 'reshard': 3, 'arrow': 2, 'dask': 2 },
    content: `### Data Processing & Sharding
- Parquet: Use pyarrow or fastparquet for columnar data
- Large datasets: dask.dataframe for out-of-core processing
- Sharding: Split by hash(key) % num_shards for even distribution
- C4 data: Common Crawl format, handle with datasets library
- Memory efficiency: Process in chunks, use generators
- Verification: Count rows before/after, checksum critical columns`,
    patterns: ['P18', 'P31'],  // MTP, RTV
  },
  theorem_proving: {
    tokens: 90,
    keywords: ['coq', 'lean', 'isabelle', 'theorem', 'proof', 'induction', 'lemma', 'tactic'],
    keywordWeights: { 'coq': 3, 'lean': 3, 'isabelle': 3, 'induction': 2 },
    content: `### Theorem Proving
- Coq: Use 'induction' tactic for recursive proofs, 'simpl' to simplify
- Lean: mathlib provides common lemmas, use 'rfl' for reflexivity
- Commutativity: Prove by induction on first argument, use IH in step case
- prove-plus-comm: Natural number addition commutativity via Peano axioms
- Tactics: intro, apply, rewrite, exact, reflexivity
- Debug: 'Show Proof' in Coq, 'trace.state' in Lean`,
    patterns: ['P5'],  // Impossible check
  },
  // OPT 2: Task-specific sections for the 5 persistently failing tasks
  chess_vision: {
    tokens: 110,
    keywords: ['chess', 'image', 'board', 'fen', 'position', 'stockfish', 'best move', 'analyze'],
    keywordWeights: { 'chessimg2pos': 4, 'stockfish': 3, 'fen': 2.5, 'best move': 2 },
    content: `### Chess Image Analysis (chess-best-move)
- Image to FEN: pip install chessimg2pos (or board_to_fen)
- Position analysis: python-chess + stockfish engine
- Workflow: image → FEN → stockfish → best move
- Install: apt-get install stockfish, pip install python-chess
- Code: import chess.engine; engine.analyse(board, chess.engine.Limit(depth=20))
- Output: UCI notation (e.g., e2e4) or SAN (e.g., e4)`,
    patterns: ['P11', 'P34'],  // PreComputed, ISP
  },
  regex_chess: {
    tokens: 100,
    keywords: ['regex', 'chess', 'pgn', 'notation', 'game', 'century', 'parse'],
    keywordWeights: { 'pgn': 3, 'game of century': 3, 'chess notation': 2.5 },
    content: `### Regex Chess (regex-chess task)
- PGN parsing: Match moves with [KQRBN]?[a-h]?[1-8]?x?[a-h][1-8](=[QRBN])?[+#]?
- Castling: O-O (kingside), O-O-O (queenside)
- Game of Century: Byrne vs Fischer 1956, know key moves
- State machine: Track board state through move sequence
- Validation: Use python-chess for legal move verification
- Edge cases: Promotions, en passant, disambiguation`,
    patterns: ['P24'],  // PCC
  },
  compression_codegolf: {
    tokens: 100,
    keywords: ['codegolf', 'minify', 'gpt2', 'small', 'bytes', 'compress', 'size'],
    keywordWeights: { 'codegolf': 3, 'gpt2': 3, 'minify': 2, 'bytes': 2 },
    content: `### Code Golf & Compression (gpt2-codegolf)
- GPT-2 codegolf: Impossible to implement full GPT-2 in <5KB
- Strategy: Pre-compute weights, use lookup tables
- Minification: Remove whitespace, use short variable names
- gpt2.c: Reference implementation exists, study structure
- Shannon limit: Cannot compress below entropy of data
- If truly impossible: Document in IMPOSSIBLE.md with proof`,
    patterns: ['P5', 'P19', 'P23'],  // Impossible, ITR+, CID
  },
  db_wal_recovery: {
    tokens: 100,
    keywords: ['wal', 'sqlite', 'recovery', 'checkpoint', 'journal', 'database', 'corrupt'],
    keywordWeights: { 'wal': 3, 'checkpoint': 3, 'db-wal-recovery': 4 },
    content: `### SQLite WAL Recovery (db-wal-recovery)
- CRITICAL: Backup WAL file FIRST before any sqlite3 command!
- cp /app/main.db-wal /tmp/wal_backup.wal
- WAL auto-checkpoints when opened - this destroys recovery data
- Parse WAL manually: struct.unpack for header (32 bytes)
- Frame format: 24-byte header + page data
- Use /tmp/wal_backup.wal for analysis, never original
- Alternative: sqlite3_recover tool from SQLite source`,
    patterns: ['P3', 'P35'],  // StateProtect, DFA
  },
};

// OPT 7: Error-to-section mapping for smarter progressive escalation
const ERROR_SECTION_MAPPING: Record<string, string[]> = {
  'permission denied': ['tools', 'output_verification'],
  'chmod': ['output_verification', 'tools'],
  'struct.unpack': ['file_formats'],
  'unpack requires': ['file_formats'],
  'no module named': ['domain_libraries', 'tools'],
  'command not found': ['tools'],
  'syntax error': ['coding', 'legacy'],
  'parse error': ['web_parsing', 'file_formats'],
  'hash': ['security', 'tools'],
  'xss': ['security', 'web_parsing'],
  'injection': ['security', 'web_parsing'],
  'git': ['git_recovery'],
  'reflog': ['git_recovery'],
  'merge conflict': ['git_recovery'],
  'detached head': ['git_recovery'],
  'parquet': ['data_processing'],
  'shard': ['data_processing'],
  'dataset': ['data_processing'],
  'coq': ['theorem_proving'],
  'lean': ['theorem_proving'],
  'induction': ['theorem_proving'],
  'chess': ['chess_vision', 'regex_chess', 'domain_libraries'],
  'stockfish': ['chess_vision', 'domain_libraries'],
  'fen': ['chess_vision'],
  'pgn': ['regex_chess'],
  'wal': ['db_wal_recovery', 'file_formats'],
  'sqlite': ['db_wal_recovery', 'file_formats'],
  'checkpoint': ['db_wal_recovery'],
  'codegolf': ['compression_codegolf'],
  'gpt2': ['compression_codegolf'],
  'minify': ['compression_codegolf'],
  'filter': ['web_parsing', 'security'],
  'html': ['web_parsing'],
  'beautifulsoup': ['web_parsing'],
};

// OPT 6: Pattern relevance by task type
const TASK_TYPE_PATTERNS: Record<string, string[]> = {
  'security': ['P10', 'P20', 'P11'],
  'file-ops': ['P9', 'P35', 'P3', 'P12'],
  'coding': ['P12', 'P16', 'P32', 'P17'],
  'debugging': ['P15', 'P13', 'P3'],
  'git-recovery': ['P22', 'P3'],
  'data-processing': ['P18', 'P31', 'P12'],
  'theorem-proving': ['P5', 'P11'],
  'legacy': ['P3', 'P35'],
  'sysadmin': ['P1', 'P8', 'P4'],
  'ml-training': ['P11', 'P33', 'P30'],
  'testing': ['P13', 'P26', 'P30'],
};

// Constants
const MS_PER_TOKEN = 1.5;
const BENEFIT_THRESHOLD = 0.1;
const RELEVANCE_THRESHOLD = 0.3;
const TIME_CRITICAL_MAX_TOKENS = 300;

// OPT 4: Calculate weighted relevance score for a section
function calculateSectionRelevance(
  instruction: string, 
  sectionConfig: { keywords: string[]; keywordWeights?: Record<string, number> }
): number {
  const lower = instruction.toLowerCase();
  let totalScore = 0;
  let matchCount = 0;
  
  for (const kw of sectionConfig.keywords) {
    if (lower.includes(kw.toLowerCase())) {
      // OPT 4: Use specificity weight if available, otherwise default to 1
      const weight = sectionConfig.keywordWeights?.[kw] || 1;
      totalScore += weight;
      matchCount++;
    }
  }
  
  // Also check global high-benefit keywords with their weights
  for (const [kw, weight] of Object.entries(HIGH_BENEFIT_KEYWORDS)) {
    if (lower.includes(kw.toLowerCase())) {
      // Check if this keyword is relevant to this section
      if (sectionConfig.keywords.some(sk => kw.includes(sk) || sk.includes(kw))) {
        totalScore += weight * 0.5; // Partial bonus for related keywords
      }
    }
  }
  
  // Normalize: max possible score is roughly keywords.length * 3 (max weight)
  const maxPossible = sectionConfig.keywords.length * 3;
  return Math.min(totalScore / Math.max(maxPossible * 0.3, 1), 1);
}

// OPT 9: Multi-category classification
export interface MultiCategoryClassification {
  primary: string;
  secondary: string[];
  confidence: number;
  keywords: string[];
}

export function classifyTaskMultiCategory(instruction: string): MultiCategoryClassification {
  const lower = instruction.toLowerCase();
  const categoryScores: Record<string, number> = {};
  const matchedKeywords: string[] = [];
  
  // Score from high-benefit keywords
  for (const [kw, weight] of Object.entries(HIGH_BENEFIT_KEYWORDS)) {
    if (lower.includes(kw.toLowerCase())) {
      matchedKeywords.push(kw);
      
      // Map keywords to categories
      if (['password', 'hash', 'crack', 'xss', 'injection', 'sanitize', 'hashcat', 'john', 'bleach', 'dompurify'].some(k => kw.includes(k))) {
        categoryScores['security'] = (categoryScores['security'] || 0) + weight;
      }
      if (['elf', 'sqlite', 'binary', 'wal', 'struct'].some(k => kw.includes(k))) {
        categoryScores['file-ops'] = (categoryScores['file-ops'] || 0) + weight;
      }
      if (['git', 'reflog', 'fsck', 'rebase'].some(k => kw.includes(k))) {
        categoryScores['git-recovery'] = (categoryScores['git-recovery'] || 0) + weight;
      }
      if (['cobol', 'fortran', 'legacy', 'mainframe'].some(k => kw.includes(k))) {
        categoryScores['legacy'] = (categoryScores['legacy'] || 0) + weight;
      }
      if (['coq', 'lean', 'theorem', 'proof', 'induction'].some(k => kw.includes(k))) {
        categoryScores['theorem-proving'] = (categoryScores['theorem-proving'] || 0) + weight;
      }
      if (['parquet', 'shard', 'reshard', 'dataset', 'arrow'].some(k => kw.includes(k))) {
        categoryScores['data-processing'] = (categoryScores['data-processing'] || 0) + weight;
      }
      if (['stockfish', 'chess', 'fen', 'pgn'].some(k => kw.includes(k))) {
        categoryScores['chess'] = (categoryScores['chess'] || 0) + weight;
      }
    }
  }
  
  // Fall back to task-classifier
  const baseClassification = classifyTaskType(instruction);
  categoryScores[baseClassification.category] = (categoryScores[baseClassification.category] || 0) + 5;
  
  // Sort by score
  const sorted = Object.entries(categoryScores)
    .sort(([, a], [, b]) => b - a);
  
  if (sorted.length === 0) {
    return {
      primary: 'coding',
      secondary: [],
      confidence: 0.5,
      keywords: matchedKeywords,
    };
  }
  
  const [primary, primaryScore] = sorted[0];
  const secondary = sorted.slice(1, 3)
    .filter(([, score]) => score >= primaryScore * 0.4)
    .map(([cat]) => cat);
  
  const maxPossible = Object.values(HIGH_BENEFIT_KEYWORDS).reduce((a, b) => a + b, 0);
  const confidence = Math.min(primaryScore / (maxPossible * 0.1), 1);
  
  return {
    primary,
    secondary,
    confidence,
    keywords: matchedKeywords,
  };
}

/**
 * Classify task type from instruction text (backward compatible)
 */
export function classifyTask(instruction: string): string {
  return classifyTaskMultiCategory(instruction).primary;
}

/**
 * Assess time pressure based on timeout and task complexity
 */
export function assessTimePressure(
  timeoutSec: number,
  taskType: string,
  difficulty: string = 'medium'
): TimePressure {
  const difficultyMultiplier: Record<string, number> = {
    easy: 0.5,
    medium: 1.0,
    hard: 2.0,
  };

  const baseDuration: Record<string, number> = {
    security: 120,
    'file-ops': 90,
    legacy: 150,
    coding: 60,
    debugging: 90,
    scheduling: 45,
    games: 30,
    'constraint-satisfaction': 60,
    'pure-logic': 90,
    reasoning: 60,
    general: 60,
    sysadmin: 120,
    'ml-training': 180,
    testing: 60,
    'git-recovery': 90,
    'data-processing': 120,
    'theorem-proving': 180,
    chess: 90,
    unknown: 60,
  };

  const expectedDuration = (baseDuration[taskType] || 60) * (difficultyMultiplier[difficulty] || 1.0);
  const ratio = timeoutSec / expectedDuration;

  if (ratio < 1.0) return 'critical';
  if (ratio < 1.3) return 'high';
  if (ratio < 1.8) return 'medium';
  return 'low';
}

/**
 * OPT 1: Get historical benefit from SQLite (persistent)
 */
export function getHistoricalBenefit(taskType: string): number {
  try {
    const db = getHistoricalDb();
    const row = db.prepare('SELECT * FROM historical_data WHERE task_type = ?').get(taskType) as HistoricalData | undefined;
    
    if (!row || row.totalAttempts < 3) {
      if (LOW_BENEFIT_CATEGORIES.has(taskType)) {
        return 0.05;
      }
      return 0.5;
    }

    const uamRate = row.uamSuccesses / Math.max(row.totalAttempts / 2, 1);
    const noUamRate = row.noUamSuccesses / Math.max(row.totalAttempts / 2, 1);

    if (noUamRate === 0) return uamRate > 0 ? 1.0 : 0.5;
    return (uamRate - noUamRate) / Math.max(uamRate, noUamRate, 0.1);
  } catch {
    // Fallback to defaults if DB fails
    if (LOW_BENEFIT_CATEGORIES.has(taskType)) {
      return 0.05;
    }
    return 0.5;
  }
}

/**
 * OPT 1: Record task outcome to SQLite (persistent)
 */
export function recordOutcome(
  taskType: string,
  usedUam: boolean,
  success: boolean,
  durationMs: number,
  modelId?: string
): void {
  try {
    const db = getHistoricalDb();
    
    // Get existing record or create new
    const existing = db.prepare('SELECT * FROM historical_data WHERE task_type = ?').get(taskType) as HistoricalData | undefined;
    
    if (existing) {
      // Update existing record
      const stmt = db.prepare(`
        UPDATE historical_data SET
          total_attempts = total_attempts + 1,
          uam_successes = uam_successes + ?,
          no_uam_successes = no_uam_successes + ?,
          avg_time_with_uam = CASE WHEN ? THEN (avg_time_with_uam * uam_successes + ?) / (uam_successes + 1) ELSE avg_time_with_uam END,
          avg_time_without_uam = CASE WHEN ? THEN (avg_time_without_uam * no_uam_successes + ?) / (no_uam_successes + 1) ELSE avg_time_without_uam END,
          last_updated = ?
        WHERE task_type = ?
      `);
      
      stmt.run(
        usedUam && success ? 1 : 0,
        !usedUam && success ? 1 : 0,
        usedUam && success ? 1 : 0,
        durationMs,
        !usedUam && success ? 1 : 0,
        durationMs,
        Date.now(),
        taskType
      );
    } else {
      // Insert new record
      const stmt = db.prepare(`
        INSERT INTO historical_data (task_type, total_attempts, uam_successes, no_uam_successes, avg_time_with_uam, avg_time_without_uam, last_updated)
        VALUES (?, 1, ?, ?, ?, ?, ?)
      `);
      
      stmt.run(
        taskType,
        usedUam && success ? 1 : 0,
        !usedUam && success ? 1 : 0,
        usedUam && success ? durationMs : 0,
        !usedUam && success ? durationMs : 0,
        Date.now()
      );
    }
  } catch (err) {
    // Log but don't throw - recording should not block execution
    console.warn('Failed to record outcome:', err);
  }
  
  // OPT 8: Also update model router fingerprints
  if (modelId) {
    const validModelIds: ModelId[] = ['glm-4.7', 'gpt-5.2', 'claude-opus-4.5', 'gpt-5.2-codex'];
    if (validModelIds.includes(modelId as ModelId)) {
      updateModelRouterFingerprint(modelId as ModelId, success, durationMs, taskType);
    }
  }
}

/**
 * OPT 10: Cache lookup for similar tasks
 */
export function lookupSemanticCache(instructionHash: string): ContextDecision | null {
  try {
    const db = getHistoricalDb();
    const row = db.prepare(`
      SELECT decision_json, success_rate 
      FROM semantic_cache 
      WHERE instruction_hash = ? AND success_rate >= 0.5
      ORDER BY success_rate DESC, use_count DESC 
      LIMIT 1
    `).get(instructionHash) as { decision_json: string; success_rate: number } | undefined;
    
    if (row) {
      // Update usage stats
      db.prepare(`
        UPDATE semantic_cache 
        SET last_used = ?, use_count = use_count + 1 
        WHERE instruction_hash = ?
      `).run(Date.now(), instructionHash);
      
      return JSON.parse(row.decision_json);
    }
  } catch {
    // Cache miss
  }
  return null;
}

/**
 * OPT 10: Store decision in semantic cache
 */
export function storeSemanticCache(
  cacheKey: string,
  instructionHash: string,
  decision: ContextDecision,
  success: boolean
): void {
  try {
    const db = getHistoricalDb();
    
    const existing = db.prepare('SELECT * FROM semantic_cache WHERE cache_key = ?').get(cacheKey);
    
    if (existing) {
      // Update success rate with exponential moving average
      db.prepare(`
        UPDATE semantic_cache SET
          decision_json = ?,
          success_rate = success_rate * 0.8 + ? * 0.2,
          last_used = ?,
          use_count = use_count + 1
        WHERE cache_key = ?
      `).run(
        JSON.stringify(decision),
        success ? 1.0 : 0.0,
        Date.now(),
        cacheKey
      );
    } else {
      db.prepare(`
        INSERT INTO semantic_cache (cache_key, instruction_hash, decision_json, success_rate, created_at, last_used, use_count)
        VALUES (?, ?, ?, ?, ?, ?, 1)
      `).run(
        cacheKey,
        instructionHash,
        JSON.stringify(decision),
        success ? 1.0 : 0.5,
        Date.now(),
        Date.now()
      );
    }
  } catch (err) {
    console.warn('Failed to store in semantic cache:', err);
  }
}

/**
 * Select relevant context sections based on task type and instruction
 * OPT 5: Returns at least 2 sections for minimal mode
 */
export function selectRelevantSections(instruction: string, taskType: string, secondaryCategories?: string[]): string[] {
  const sectionsWithScores: Array<{ name: string; score: number; patterns?: string[] }> = [];

  for (const [name, config] of Object.entries(CONTEXT_SECTIONS)) {
    const score = calculateSectionRelevance(instruction, config);
    if (score >= RELEVANCE_THRESHOLD) {
      sectionsWithScores.push({ name, score, patterns: config.patterns });
    }
  }

  // Sort by relevance score descending
  sectionsWithScores.sort((a, b) => b.score - a.score);

  const sections = sectionsWithScores.map(s => s.name);
  
  // Add default sections for certain task types if not already included
  const addIfMissing = (section: string) => {
    if (!sections.includes(section)) sections.push(section);
  };
  
  // Primary category defaults
  if (taskType === 'security') addIfMissing('security');
  if (taskType === 'file-ops') addIfMissing('file_formats');
  if (taskType === 'legacy') addIfMissing('legacy');
  if (taskType === 'git-recovery') addIfMissing('git_recovery');
  if (taskType === 'data-processing') addIfMissing('data_processing');
  if (taskType === 'theorem-proving') addIfMissing('theorem_proving');
  if (taskType === 'chess') {
    addIfMissing('chess_vision');
    addIfMissing('domain_libraries');
  }
  
  // OPT 9: Add sections for secondary categories too
  if (secondaryCategories) {
    for (const cat of secondaryCategories) {
      if (cat === 'security') addIfMissing('security');
      if (cat === 'file-ops') addIfMissing('file_formats');
      if (cat === 'git-recovery') addIfMissing('git_recovery');
    }
  }

  return sections;
}

/**
 * Calculate estimated overhead for given sections in milliseconds
 */
export function calculateOverhead(sections: string[]): number {
  let totalTokens = 0;
  for (const section of sections) {
    totalTokens += CONTEXT_SECTIONS[section]?.tokens || 0;
  }
  return totalTokens * MS_PER_TOKEN;
}

/**
 * OPT 6: Get relevant patterns for task type
 */
export function getRelevantPatterns(taskType: string, sections: string[]): string[] {
  const patterns = new Set<string>();
  
  // From task type
  const typePatterns = TASK_TYPE_PATTERNS[taskType] || [];
  for (const p of typePatterns) patterns.add(p);
  
  // From selected sections
  for (const section of sections) {
    const sectionConfig = CONTEXT_SECTIONS[section];
    if (sectionConfig?.patterns) {
      for (const p of sectionConfig.patterns) patterns.add(p);
    }
  }
  
  return Array.from(patterns);
}

/**
 * Main decision function: determine optimal context level using hybrid approach
 */
export function decideContextLevel(
  instruction: string,
  metadata: TaskMetadata = {}
): ContextDecision {
  // OPT 9: Use multi-category classification
  const multiClass = classifyTaskMultiCategory(instruction);
  const taskType = multiClass.primary;
  const timeoutSec = metadata.timeout_sec || 300;
  const difficulty = metadata.difficulty || 'medium';

  // Factor 1: Task classification - skip for pure reasoning
  if (LOW_BENEFIT_CATEGORIES.has(taskType)) {
    return {
      level: 'none',
      sections: [],
      reason: `Task type '${taskType}' is pure reasoning - UAM adds no benefit`,
      estimatedOverheadMs: 0,
      taskType,
      timePressure: 'low',
      historicalBenefit: 0,
      secondaryCategories: multiClass.secondary,
    };
  }

  // Factor 2: Time pressure assessment
  const timePressure = assessTimePressure(timeoutSec, taskType, difficulty);

  // Factor 3: Historical benefit (now from SQLite - OPT 1)
  const historicalBenefit = metadata.historical_uam_benefit ?? getHistoricalBenefit(taskType);

  // Factor 4: Check if historical data suggests skipping UAM
  if (historicalBenefit < BENEFIT_THRESHOLD && !ALWAYS_BENEFIT_CATEGORIES.has(taskType)) {
    return {
      level: 'none',
      sections: [],
      reason: `Low historical benefit (${(historicalBenefit * 100).toFixed(1)}%) for ${taskType}`,
      estimatedOverheadMs: 0,
      taskType,
      timePressure,
      historicalBenefit,
      secondaryCategories: multiClass.secondary,
    };
  }

  // Factor 5: Critical time pressure - skip UAM
  if (timePressure === 'critical') {
    return {
      level: 'none',
      sections: [],
      reason: 'Critical time pressure - skipping UAM to avoid timeout',
      estimatedOverheadMs: 0,
      taskType,
      timePressure,
      historicalBenefit,
      secondaryCategories: multiClass.secondary,
    };
  }

  // Factor 6: Select relevant sections (OPT 9: including secondary categories)
  const relevantSections = selectRelevantSections(instruction, taskType, multiClass.secondary);
  const estimatedOverhead = calculateOverhead(relevantSections);
  
  // OPT 6: Get relevant patterns
  const relevantPatterns = getRelevantPatterns(taskType, relevantSections);

  // Factor 7: Check if overhead fits within time budget
  const overheadRatio = estimatedOverhead / (timeoutSec * 1000);

  // Time-critical tasks (<120s): cap overhead to TIME_CRITICAL_MAX_TOKENS
  if (timeoutSec < 120) {
    const cappedSections: string[] = [];
    let tokenBudget = TIME_CRITICAL_MAX_TOKENS;
    for (const section of relevantSections) {
      const sectionTokens = CONTEXT_SECTIONS[section]?.tokens || 0;
      if (tokenBudget - sectionTokens >= 0) {
        cappedSections.push(section);
        tokenBudget -= sectionTokens;
      }
    }
    return {
      level: cappedSections.length > 0 ? 'minimal' : 'none',
      sections: cappedSections,
      reason: `Time-critical task (<120s) - capped to ${TIME_CRITICAL_MAX_TOKENS} tokens`,
      estimatedOverheadMs: calculateOverhead(cappedSections),
      taskType,
      timePressure,
      historicalBenefit,
      secondaryCategories: multiClass.secondary,
      relevantPatterns,
    };
  }

  // OPT 5: Use 2 sections instead of 1 for minimal mode
  if (timePressure === 'high' || overheadRatio > 0.1) {
    const minimalSections = relevantSections.slice(0, 2); // Changed from 1 to 2
    return {
      level: 'minimal',
      sections: minimalSections,
      reason: `High time pressure - using minimal context (${minimalSections.join(', ') || 'best_practices'})`,
      estimatedOverheadMs: calculateOverhead(minimalSections),
      taskType,
      timePressure,
      historicalBenefit,
      secondaryCategories: multiClass.secondary,
      relevantPatterns,
    };
  }

  // Default: Full context for everything else
  return {
    level: 'full',
    sections: relevantSections.length > 0 ? relevantSections : ['coding'],
    reason: `Full context for ${taskType} task (${timePressure} pressure)`,
    estimatedOverheadMs: estimatedOverhead,
    taskType,
    timePressure,
    historicalBenefit,
    secondaryCategories: multiClass.secondary,
    relevantPatterns,
  };
}

/**
 * Generate context string based on decision
 */
export function generateContext(decision: ContextDecision): string {
  if (decision.level === 'none' || decision.sections.length === 0) {
    return '';
  }

  const contextParts: string[] = ['## UAM Memory Context\n'];

  for (const section of decision.sections) {
    const sectionConfig = CONTEXT_SECTIONS[section];
    if (sectionConfig) {
      contextParts.push(sectionConfig.content);
    }
  }
  
  // OPT 6: Add relevant patterns hint
  if (decision.relevantPatterns && decision.relevantPatterns.length > 0) {
    contextParts.push(`\n### Relevant Patterns: ${decision.relevantPatterns.join(', ')}`);
  }

  return contextParts.join('\n');
}

/**
 * OPT 7: Enhanced progressive context strategy with error-to-section mapping
 */
export function getProgressiveContextLevels(
  instruction: string,
  initialError: string,
  metadata: TaskMetadata = {}
): ContextLevel[] {
  const decision = decideContextLevel(instruction, metadata);

  if (decision.level === 'none' && LOW_BENEFIT_CATEGORIES.has(decision.taskType)) {
    return ['none'];
  }

  const errorLower = initialError.toLowerCase();
  
  // OPT 7: Check error-to-section mapping for targeted escalation
  let suggestedSections: string[] = [];
  for (const [errorPattern, sections] of Object.entries(ERROR_SECTION_MAPPING)) {
    if (errorLower.includes(errorPattern)) {
      suggestedSections.push(...sections);
    }
  }
  
  // Standard context-might-help checks
  const contextMightHelp =
    suggestedSections.length > 0 ||
    errorLower.includes('unknown') ||
    errorLower.includes('how to') ||
    errorLower.includes('what is') ||
    errorLower.includes('command not found') ||
    errorLower.includes('invalid syntax') ||
    errorLower.includes('format') ||
    errorLower.includes('parse');

  if (!contextMightHelp) {
    return [decision.level];
  }

  // Progressive escalation based on starting point
  switch (decision.level) {
    case 'none':
      return ['none', 'minimal', 'full'];
    case 'minimal':
      return ['minimal', 'full'];
    case 'full':
      return ['full'];
    default:
      return ['none', 'minimal', 'full'];
  }
}

/**
 * OPT 7: Get additional sections to add based on error analysis
 */
export function getSectionsForError(error: string): string[] {
  const errorLower = error.toLowerCase();
  const sections = new Set<string>();
  
  for (const [errorPattern, sectionList] of Object.entries(ERROR_SECTION_MAPPING)) {
    if (errorLower.includes(errorPattern)) {
      for (const section of sectionList) {
        sections.add(section);
      }
    }
  }
  
  return Array.from(sections);
}

/**
 * Export configuration for Python agent integration
 */
export function exportConfigForPython(instruction: string, metadata: TaskMetadata = {}): string {
  const decision = decideContextLevel(instruction, metadata);
  const context = generateContext(decision);

  return JSON.stringify(
    {
      level: decision.level,
      sections: decision.sections,
      reason: decision.reason,
      estimatedOverheadMs: decision.estimatedOverheadMs,
      taskType: decision.taskType,
      timePressure: decision.timePressure,
      historicalBenefit: decision.historicalBenefit,
      secondaryCategories: decision.secondaryCategories,
      relevantPatterns: decision.relevantPatterns,
      context,
    },
    null,
    2
  );
}

/**
 * OPT 8: Get model fingerprint for routing integration
 */
export function getModelFingerprintForTask(taskType: string): { recommended: ModelId; reason: string } | null {
  // Check per-category success rates from model router
  const models: ModelId[] = ['claude-opus-4.5', 'gpt-5.2', 'glm-4.7', 'gpt-5.2-codex'];
  
  let bestModel: ModelId = 'claude-opus-4.5';
  let bestScore = 0;
  
  for (const modelId of models) {
    const fp = getModelFingerprint(modelId);
    if (fp && fp.categoryStats?.[taskType]) {
      const stats = fp.categoryStats[taskType];
      if (stats.attempts >= 3) {
        const rate = stats.successes / stats.attempts;
        if (rate > bestScore) {
          bestScore = rate;
          bestModel = modelId;
        }
      }
    }
  }
  
  if (bestScore > 0) {
    return {
      recommended: bestModel,
      reason: `${bestModel} has ${(bestScore * 100).toFixed(0)}% success rate for ${taskType} tasks`,
    };
  }
  
  return null;
}

/**
 * Close database connection (for cleanup)
 */
export function closeHistoricalDb(): void {
  if (historicalDb) {
    historicalDb.close();
    historicalDb = null;
  }
}

// Export main interface
export const HybridAdaptiveContext = {
  classifyTask,
  classifyTaskMultiCategory,
  assessTimePressure,
  getHistoricalBenefit,
  recordOutcome,
  decideContextLevel,
  generateContext,
  selectRelevantSections,
  calculateOverhead,
  getProgressiveContextLevels,
  getSectionsForError,
  getRelevantPatterns,
  exportConfigForPython,
  lookupSemanticCache,
  storeSemanticCache,
  getModelFingerprintForTask,
  closeHistoricalDb,
};

export default HybridAdaptiveContext;
